# Weed-Sentiment-Analysis-NLP

Project Idea: Using 2020's geolocation-specific Twitter data, we aim to gauge Californians' sentiments on cannabis. We'll compile a dataset of dispensary density and incorporate socioeconomic metrics like education and poverty rates (all county-level, 2020, in California). After deriving sentiment scores, we'll merge the data into one data frame. Using regression analyses, we'll identify trends and present the findings through visualizations. The study examines if dispensary concentration and socioeconomic factors shape local attitudes toward cannabis.

Description of datasets: The "ActiveRetailer_Cannabis.xlsx" from the California Department of Cannabis Control contains data on 1,653 licensed retailers from 2018-2024, including license details (e.g. License type, Issuance date, and Expiration date ) and store locations. This will be used for a 2020 retailer panel dataset and dispensary density map; Cannabis-related tweets in 2020: Targeting 10,000 cleaned tweets with cannabis keywords from California accounts, this dataset will aid in analyzing sentiment and regional cannabis trends.; CA County Socioeconomic (2020): Sourced from the U.S. Census Bureau, this dataset provides insights on county-level poverty and education.

## Twitter Data Collection 

To retrieve relevant Twitter data for this research, we customized a web scraping script developed using Python. The first thing we need to do is to establish a search endpoint: the script targets the GraphQL endpoint of Twitter for search queries. GraphQL is a query language that allows precise fetching of required data. To initialize, we created the `Gtwitter` class, once instantiated, is set with a `max_page` parameter, defining the maximum number of pages to scrape. This prevents over-scraping and limits the volume of data retrieved. The `getblog` function within the class handles the communication with Twitter's server. It prepares a request with necessary headers, which include authorization, user-agent, and other pertinent information. At the same time randomized cookies are chosen to simulate different user sessions. Error handling is also incorporated to manage rate limits imposed by Twitter. When faced with a rate limit, the script would pause and retry. To parse the data, the retrieved data is processed using the `parse` function. This function navigates through the JSON response to extract individual tweet details. Details such as tweet creation date, username, tweet content, retweet count, favorite count, and several others are extracted. The script particularly focuses on tweets with the keyword 'California' and its associated city saves those details into a temporary file for subsequent processing. Only tweets containing keywords ‘Weed’ are considered to ensure relevance to the study. Relevant tweet details are stored in a temporary file named "tepm.txt". Each tweet's details are stored as a JSON object on a new line. The script incorporates a utility function `generXlsx` that reads the saved tweet details and converts them into an Excel file format using the pandas library. This ensures easy readability and further data analysis using traditional data analysis tools. The `changev` function is used for transforming Twitter's timestamp into a standardized date-time format. The `blockdata` function orchestrates the overall flow of data retrieval and parsing. It handles pagination by analyzing the response to determine the cursor for the next page of tweets. The `getdaterange` function is a utility function to get a list of dates between a start and end date. 

## Twitter Data Preprocessing 

URL Removal: All URLs embedded in the tweets were removed to ensure the content's clarity. This was achieved by identifying and replacing any sequences starting with 'http' and followed by one or more non-space characters with an empty string.

Case Normalization: To maintain consistency and to avoid redundancy due to case variations, all the text was converted to lowercase. This ensures that words like 'Twitter' and 'twitter' are treated the same.

Special Character and Number Removal: Any special characters and numbers that might not add significant meaning to the content were stripped off. This aids in focusing only on the words and their semantics.

Whitespace Reduction: Extra spaces, whether at the beginning, middle, or end of the tweets, were removed. This not only streamlines the data but also makes sure the content is consistent and tidy.

The cleaned content is then stored in a new column called 'Cleaned_Content', thereby retaining the original content for reference or further analysis.

## Use of NLP models
We mainly employed two models for this prediction task. VADER, a lexicon and rule-based sentiment analysis tool that is specifically designed to recognize sentiment in text, especially for social media and short-text content. The other, BERT, a state-of-the-art machine learning model for natural language processing tasks. 

For VADER, each cleaned tweet in the dataset, the `polarity_scores` method of the analyzer was invoked. This method calculates a sentiment score for the given text based on the words it contains and the rules defined in VADER's lexicon. From the sentiment scores returned by VADER, the 'compound' score was extracted. The compound score is a single decimal number ranging from -1 to 1, representing the overall sentiment of the text. A positive compound score indicates a positive sentiment, a negative score indicates negative sentiment, and a score close to zero suggests a neutral sentiment.

For BERT, We first tokenized the data. This step converts textual content into numerical format, which can be fed into the BERT model. We then split the dataset into training (80%) and validation (20%) sets. The number of labels for classification was set based on the unique sentiment labels in the dataset. The AdamW optimizer was chosen for model optimization. A learning rate scheduler was also defined to adjust the learning rate over training epochs. The loss function was defined as the CrossEntropyLoss. The model was trained for 3 epochs. During each epoch, the model parameters were updated using the training data, and the training loss was calculated. The model was then evaluated on the validation set, and the validation loss was computed. Both training and validation losses were printed at the end of each epoch to monitor the model's performance.

## Model Accuracy Evaluation and Adjustment 
To gauge the accuracy of the model, each of the teammates labeled 250 tweets(1000 total) as either ‘Positive’, ‘Negative’ or ‘Neutral’. Then we convert the BERT and VADER score to match the same sentiment labels. By calculating the number of correct predictions/total predictions we get roughly 50% accuracy for both BERT and VADER models.  The result we get is clearly not ideal, to combat this, we used Transfer Learning Technique on the BERT model. 

## Model fine-tuning and evaluation
The rationale for using Transfer Learning Technique is twofold: firstly, the pre-trained model brings a wealth of generalized language understanding, which might be challenging to capture with limited domain-specific data. Secondly, the fine-tuning process hones this general understanding to the idiosyncrasies of the target task, ensuring the model is aptly specialized. By employing this method, we are able to increase the prediction accuracy by 10 percent on the labeled dataset and we believe if we have more labeled data and increase the epoch, we would get an even better accuracy.

## Sentiment distribution across counties 
We aggregated the tweet sentiments by county. This step allowed for the calculation of average sentiment scores for both VADER and BERT for each county, providing a high-level view of sentiment distribution across the state.

To enhance the depth of the analysis, we introduced a secondary dataset: a CSV file containing population figures for California counties spanning from 2018 to 2022. The county names in this dataset were refined by removing the descriptor "County" for a cleaner representation and to facilitate data merging. Using this population data, we calculated the average 5-year population for each county, capturing a representative figure of the population over recent years.

Merging the sentiment scores with the population data, we computed weighted sentiment scores for both the VADER and BERT models. This weighting was based on the average 5-year population for each county. This step ensured that the sentiment scores were adjusted to reflect the population size of each county, emphasizing the significance of sentiment in more populous regions. Following this, we visualized the weighted sentiment scores, highlighting how the sentiment landscape shifts when considering population magnitude.
